{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "tutorial_helloworld_DQN_DDPG_PPO.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1gUG3OCJ5GS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Demo: ElegantRL_HelloWorld_tutorial (DQN --> DDPG --> PPO)\n",
    "\n",
    "We suggest to following this order to quickly learn about RL:\n",
    "- DQN (Deep Q Network), a basic RL algorithms in discrete action space.\n",
    "- DDPG (Deep Deterministic Policy Gradient), a basic RL algorithm in continuous action space.\n",
    "- PPO (Proximal Policy Gradient), a widely used RL algorithms in continuous action space.\n",
    "\n",
    "If you have any suggestion about ElegantRL Helloworld, you can discuss them in [ElegantRL issues/135: Suggestions for elegant_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/issues/135), and we will keep an eye on this issue.\n",
    "ElegantRL's code, especially the Helloworld, really needs a lot of feedback to be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbamGVHC3AeW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Part 1: Install ElegantRL**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U35bhkUqOqbS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "42c4d1a1-3e31-40d4-de5a-511dad532915",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# install elegantrl library\n",
    "!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
      "  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to c:\\users\\zdl\\appdata\\local\\temp\\pip-req-build-zubcy210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/AI4Finance-LLC/ElegantRL.git 'C:\\Users\\zdl\\AppData\\Local\\Temp\\pip-req-build-zubcy210'\n",
      "  ERROR: Error [WinError 2] 系统找不到指定的文件。 while executing command git clone -q https://github.com/AI4Finance-LLC/ElegantRL.git 'C:\\Users\\zdl\\AppData\\Local\\Temp\\pip-req-build-zubcy210'\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## **Part 2: Import ElegantRL helloworld**\n",
    "\n",
    "We hope that the `ElegantRL Helloworld` would help people who want to learn about reinforcement learning to quickly run a few introductory examples.\n",
    "- **Less lines of code**. (code lines <1000)\n",
    "- **Less packages requirements**. (only `torch` and `gym` )\n",
    "- **keep a consistent style with the full version of ElegantRL**.\n",
    "\n",
    "![File_structure of ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/figs/File_structure.png)\n",
    "\n",
    "One sentence summary: an agent `agent.py` with Actor-Critic networks `net.py` is trained `run.py` by interacting with an environment `env.py`.\n",
    "\n",
    "\n",
    "In this tutorial, we only need to download the directory from [elegantrl_helloworld](https://github.com/AI4Finance-Foundation/ElegantRL/tree/master/elegantrl_helloworld) using the following code.\n",
    "\n",
    "The files in `elegantrl_helloworld` including:\n",
    "`config.py`, `agent.py`, `net.py`, `env.py`, `run.py`"
   ],
   "metadata": {
    "id": "zJPivVxHMrAt",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -r -f /content/elegantrl_helloworld  # remove if the directory exists\n",
    "!wget https://github.com/AI4Finance-Foundation/ElegantRL/raw/master/elegantrl_helloworld -P /content/"
   ],
   "metadata": {
    "id": "sw_gE-IpovQ4",
    "outputId": "b291f901-7b68-41ea-c261-96b8d6fe4fdc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "'wget' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
    "from elegantrl_helloworld.env import get_gym_env_args\n",
    "from elegantrl_helloworld.config import Arguments"
   ],
   "metadata": {
    "id": "nweGpiR1M0yA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVdmpnK_3Zcn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Part 3: Train DQN on discreted action space task.**\n",
    "\n",
    "Train DQN on [**Discreted action** space task `CartPole`](https://gym.openai.com/envs/CartPole-v1/)\n",
    "\n",
    "You can see [/elegantrl_helloworld/config.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/elegantrl_helloworld/config.py) to get more information about hyperparameter.\n",
    "\n",
    "```\n",
    "class Arguments:\n",
    "    def __init__(self, agent_class, env_func=None, env_args=None):\n",
    "        self.env_num = self.env_args['env_num']  # env_num = 1. In vector env, env_num > 1.\n",
    "        self.max_step = self.env_args['max_step']  # the max step of an episode\n",
    "        self.env_name = self.env_args['env_name']  # the env name. Be used to set 'cwd'.\n",
    "        self.state_dim = self.env_args['state_dim']  # vector dimension (feature number) of state\n",
    "        self.action_dim = self.env_args['action_dim']  # vector dimension (feature number) of action\n",
    "        self.if_discrete = self.env_args['if_discrete']  # discrete or continuous action space\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from elegantrl_helloworld.agent import AgentDQN\n",
    "agent_class = AgentDQN\n",
    "env_name = \"CartPole-v0\"\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40)  # Block warning\n",
    "env = gym.make(env_name)\n",
    "env_func = gym.make\n",
    "env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "'''reward shaping'''\n",
    "args.reward_scale = 2 ** 0  # an approximate target reward usually be closed to 256\n",
    "args.gamma = 0.97  # discount factor of future rewards\n",
    "\n",
    "'''network update'''\n",
    "args.target_step = args.max_step * 2  # collect target_step, then update network\n",
    "args.net_dim = 2 ** 7  # the middle layer dimension of Fully Connected Network\n",
    "args.num_layer = 3  # the layer number of MultiLayer Perceptron, `assert num_layer >= 2`\n",
    "args.batch_size = 2 ** 7  # num of transitions sampled from replay buffer.\n",
    "args.repeat_times = 2 ** 0  # repeatedly update network using ReplayBuffer to keep critic's loss small\n",
    "args.explore_rate = 0.25  # epsilon-greedy for exploration.\n",
    "\n",
    "'''evaluate'''\n",
    "args.eval_gap = 2 ** 5  # number of times that get episode return\n",
    "args.eval_times = 2 ** 3  # number of times that get episode return\n",
    "args.break_step = int(8e4)  # break training if 'total_step > break_step'"
   ],
   "metadata": {
    "id": "AAPdjovQrTpE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "df46a9ae-4d8c-4836-b471-f755282a5393",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_args = {'env_num': 1,\n",
      "            'env_name': 'CartPole-v0',\n",
      "            'max_step': 200,\n",
      "            'state_dim': 4,\n",
      "            'action_dim': 2,\n",
      "            'if_discrete': True}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m env_func \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake\n\u001B[0;32m      9\u001B[0m env_args \u001B[38;5;241m=\u001B[39m get_gym_env_args(env, if_print\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 11\u001B[0m args \u001B[38;5;241m=\u001B[39m \u001B[43mArguments\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124;03m'''reward shaping'''\u001B[39;00m\n\u001B[0;32m     14\u001B[0m args\u001B[38;5;241m.\u001B[39mreward_scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0\u001B[39m  \u001B[38;5;66;03m# an approximate target reward usually be closed to 256\u001B[39;00m\n",
      "File \u001B[1;32mC:\\My program\\ElegantRL-master\\elegantrl_helloworld\\config.py:16\u001B[0m, in \u001B[0;36mArguments.__init__\u001B[1;34m(self, env_func, env_args, hyp)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_func \u001B[38;5;241m=\u001B[39m env_func  \u001B[38;5;66;03m# env = env_func(*env_args)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_args \u001B[38;5;241m=\u001B[39m env_args  \u001B[38;5;66;03m# env = env_func(*env_args)\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menv_num\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m  \u001B[38;5;66;03m# env_num = 1. In vector env, env_num > 1.\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_step\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# the max step of an episode\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menv_name\u001B[39m\u001B[38;5;124m'\u001B[39m]  \u001B[38;5;66;03m# the env name. Be used to set 'cwd'.\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'function' object is not subscriptable"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose gpu id `0` using `args.learner_gpu = 0`. Set as `-1` or GPU is unavaliable, the training program will choose CPU automatically.\n",
    "\n",
    "- The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200) \n",
    "- The cumulative returns of task_name is ∈ (min score, (score of random action, target score), max score)."
   ],
   "metadata": {
    "id": "Rq5LPOH2B0aw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "args.learner_gpus = -1\n",
    "\n",
    "train_agent(args)\n",
    "evaluate_agent(args)\n",
    "print('| The cumulative returns of CartPole-v0  is ∈ (0, (1, 195), 200)')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7SBwVAkA8lA",
    "outputId": "a0385b35-5886-4a96-c55c-3d19606f9bb8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train DQN on [**Discreted action** space env `LunarLander`](https://gym.openai.com/envs/LunarLander-v2/)\n",
    "\n",
    "**You can pass and run codes below.**. Because DQN takes over 6000 seconds for training. It is too slow. (DuelingDoubleDQN taks less than 1000 second for training on LunarLander-v2 task.)\n",
    "\n",
    "And there are many other DQN variance algorithms which get higher cumulative returns and takes less time for training. See [examples/demo_DQN_Dueling_Double_DQN.py](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/examples/demo_DQN_Dueling_Double_DQN.py)"
   ],
   "metadata": {
    "id": "qK21xTxnHGOp",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from elegantrl_helloworld.agent import AgentDQN\n",
    "agent_class = AgentDQN\n",
    "env_name = \"LunarLander-v2\"\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40)  # Block warning\n",
    "env = gym.make(env_name)\n",
    "env_func = gym.make\n",
    "env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "'''reward shaping'''\n",
    "args.reward_scale = 2 ** 0\n",
    "args.gamma = 0.99\n",
    "\n",
    "'''network update'''\n",
    "args.target_step = args.max_step\n",
    "args.net_dim = 2 ** 7\n",
    "args.num_layer = 3\n",
    "\n",
    "args.batch_size = 2 ** 6\n",
    "\n",
    "args.repeat_times = 2 ** 0\n",
    "args.explore_noise = 0.125\n",
    "\n",
    "'''evaluate'''\n",
    "args.eval_gap = 2 ** 7\n",
    "args.eval_times = 2 ** 4\n",
    "args.break_step = int(4e5)  # LunarLander needs a larger `break_step`\n",
    "\n",
    "args.learner_gpus = -1  # denotes use CPU\n",
    "train_agent(args)\n",
    "evaluate_agent(args)\n",
    "print('| The cumulative returns of LunarLander-v2 is ∈ (-1800, (-600, 200), 340)')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yH91VA17Hcsn",
    "outputId": "fc4e96cb-9000-4ead-d899-ff0722218929",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Part 4: Train DDPG on continuous action space task.**\n",
    "\n",
    "Train DDPG on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/)\n",
    "\n",
    "We show a cunstom env in [elegantrl_helloworld/env.py `class PendulumEnv`](https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/elegantrl_helloworld/env.py#L19-L23)\n",
    "\n",
    "OpenAI Pendulum env set its action space as (-2, +2). It is bad. We suggest that adjust action space to (-1, +1) when designing your own env.\n"
   ],
   "metadata": {
    "id": "z2Ik5cDoyPGU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from elegantrl_helloworld.config import Arguments\n",
    "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
    "from elegantrl_helloworld.env import get_gym_env_args\n",
    "from elegantrl_helloworld.agent import AgentDDPG\n",
    "agent_class = AgentDDPG\n",
    "\n",
    "from elegantrl_helloworld.env import PendulumEnv\n",
    "env = PendulumEnv('Pendulum-v0')  # PendulumEnv('Pendulum-v1')\n",
    "env_func = PendulumEnv\n",
    "env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "'''reward shaping'''\n",
    "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
    "args.gamma = 0.97\n",
    "\n",
    "'''network update'''\n",
    "args.target_step = args.max_step * 2\n",
    "args.net_dim = 2 ** 7\n",
    "args.batch_size = 2 ** 7\n",
    "args.repeat_times = 2 ** 0\n",
    "args.explore_noise = 0.1\n",
    "\n",
    "'''evaluate'''\n",
    "args.eval_gap = 2 ** 6\n",
    "args.eval_times = 2 ** 3\n",
    "args.break_step = int(1e5)\n",
    "\n",
    "args.learner_gpus = -1  # denotes use CPU\n",
    "train_agent(args)\n",
    "evaluate_agent(args)\n",
    "print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwkZXiHtyV6f",
    "outputId": "0e7c8c26-9b4b-42e3-de2a-9da7f76bf670",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3n8zcgcn14uq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Part 5: Train PPO on continuous action space task.**\n",
    "\n",
    "Train PPO on [**Continuous action** space env `Pendulum`](https://gym.openai.com/envs/Pendulum-v0/). \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E03f6cTeajK4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0e62173c-b6af-4b36-9073-875c2f72fd73",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from elegantrl_helloworld.config import Arguments\n",
    "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
    "from elegantrl_helloworld.env import get_gym_env_args\n",
    "from elegantrl_helloworld.agent import AgentPPO\n",
    "agent_class = AgentPPO\n",
    "\n",
    "from elegantrl_helloworld.env import PendulumEnv\n",
    "env = PendulumEnv()\n",
    "env_func = PendulumEnv\n",
    "env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "'''reward shaping'''\n",
    "args.reward_scale = 2 ** -1  # RewardRange: -1800 < -200 < -50 < 0\n",
    "args.gamma = 0.97\n",
    "\n",
    "'''network update'''\n",
    "args.target_step = args.max_step * 8\n",
    "args.net_dim = 2 ** 7\n",
    "args.num_layer = 2\n",
    "args.batch_size = 2 ** 8\n",
    "args.repeat_times = 2 ** 5\n",
    "\n",
    "'''evaluate'''\n",
    "args.eval_gap = 2 ** 6\n",
    "args.eval_times = 2 ** 3\n",
    "args.break_step = int(8e5)\n",
    "\n",
    "args.learner_gpus = -1\n",
    "train_agent(args)\n",
    "evaluate_agent(args)\n",
    "print('| The cumulative returns of Pendulum-v1 is ∈ (-1600, (-1400, -200), 0)')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train PPO on [**Continuous action** space env `LunarLanderContinuous`](https://gym.openai.com/envs/LunarLanderContinuous-v2/)"
   ],
   "metadata": {
    "id": "rcFcUkwfzHLE",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from elegantrl_helloworld.config import Arguments\n",
    "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
    "from elegantrl_helloworld.env import get_gym_env_args\n",
    "from elegantrl_helloworld.agent import AgentPPO\n",
    "agent_class = AgentPPO\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "\n",
    "import gym\n",
    "env = gym.make(env_name)\n",
    "env_func = gym.make\n",
    "env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "'''reward shaping'''\n",
    "args.gamma = 0.99\n",
    "args.reward_scale = 2 ** -1\n",
    "\n",
    "'''network update'''\n",
    "args.target_step = args.max_step * 8\n",
    "args.num_layer = 3\n",
    "args.batch_size = 2 ** 7\n",
    "args.repeat_times = 2 ** 4\n",
    "args.lambda_entropy = 0.04\n",
    "\n",
    "'''evaluate'''\n",
    "args.eval_gap = 2 ** 6\n",
    "args.eval_times = 2 ** 5\n",
    "args.break_step = int(4e5)\n",
    "\n",
    "args.learner_gpus = -1\n",
    "train_agent(args)\n",
    "evaluate_agent(args)\n",
    "print('| The cumulative returns of LunarLanderContinuous-v2 is ∈ (-1800, (-300, 200), 310+)')"
   ],
   "metadata": {
    "id": "9WCAcmIfzGyE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "03513c3c-e5b8-4f4a-b4f2-00ef753d7c95",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1j5kLHF2dhJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train PPO on [**Continuous action** space env `BipedalWalker`](https://gym.openai.com/envs/BipedalWalker-v2/)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KGOPSD6da23k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8b126d76-ea1d-40bb-f00c-05b59c1f9669",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from elegantrl_helloworld.config import Arguments\n",
    "from elegantrl_helloworld.run import train_agent, evaluate_agent\n",
    "from elegantrl_helloworld.env import get_gym_env_args\n",
    "from elegantrl_helloworld.agent import AgentPPO\n",
    "agent_class = AgentPPO\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "\n",
    "import gym\n",
    "env = gym.make(env_name)\n",
    "env_func = gym.make\n",
    "env_args = get_gym_env_args(env, if_print=True)\n",
    "\n",
    "args = Arguments(agent_class, env_func, env_args)\n",
    "\n",
    "'''reward shaping'''\n",
    "args.reward_scale = 2 ** -1\n",
    "args.gamma = 0.98\n",
    "\n",
    "'''network update'''\n",
    "args.target_step = args.max_step\n",
    "args.net_dim = 2 ** 8\n",
    "args.num_layer = 3\n",
    "args.batch_size = 2 ** 8\n",
    "args.repeat_times = 2 ** 4\n",
    "\n",
    "'''evaluate'''\n",
    "args.eval_gap = 2 ** 6\n",
    "args.eval_times = 2 ** 4\n",
    "args.break_step = int(1e6)\n",
    "\n",
    "args.learner_gpus = -1\n",
    "train_agent(args)\n",
    "evaluate_agent(args)\n",
    "print('| The cumulative returns of BipedalWalker-v3 is ∈ (-150, (-100, 280), 320+)')\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}